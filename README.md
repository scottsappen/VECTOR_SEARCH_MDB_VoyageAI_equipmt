# üîé VECTOR_SEARCH with Flink + Confluent Cloud + MongoDB + VoyageAI ‚Äî equipment maintenance mock dataset

A hands-on, copy‚Äëpaste friendly walkthrough that highlights **VoyageAI text embeddings** and **Confluent Flink SQL `VECTOR_SEARCH`**, using a mix of **click‚Äëops** (web UIs) and **simple CLI/scripts**. No Docker. No sprawling infra. Just enough to learn quickly and extend later.

<br>

## üß≠ Overview

As agentic AI rises, **RAG** (retrieval‚Äëaugmented generation) still does the heavy lifting for grounding answers in recent, relevant data. This repo shows how **streaming + embeddings + vector search** fit together with:
- **Flink SQL** on **Confluent Cloud** (managed Flink)
- **VoyageAI** for fast, high‚Äëquality text embeddings
- **MongoDB Atlas** for a **vector index** and semantic search

**Who is this for?** Builders who want a *minimal* but real demo of vector search and a path to future agentic workflows‚Äîwithout standing up complex services.

**You will:**  
- Wire up Flink SQL with VoyageAI embeddings and MongoDB Atlas vector search  
- Run end‚Äëto‚Äëend queries from a small Python producer into Flink SQL  
- Explore `VECTOR_SEARCH` directly in SQL
- Keep everything readable and easy to swap for your own data later

> Assumptions: macOS (or equivalent), Python installed, and you‚Äôre comfortable with `pip install python-dotenv` and basic terminal use.

‚úÖ Prereqs (Python & OS)
- Python: 3.10+
- macOS should work out of the box

Something like this, from the repo root
- confluent-kafka[avro] ‚Äî Kafka producer + Schema Registry Avro serializer
- pymongo ‚Äî MongoDB client
- voyageai ‚Äî VoyageAI embeddings client
- python-dotenv ‚Äî loads your .env file

```bash
python3 -m venv .venv
source .venv/bin/activate
python -m pip install -U pip
pip install "confluent-kafka[avro]" pymongo voyageai python-dotenv
```

<br>

## üß† What You Will Accomplish

- Spin up a **Flink SQL** pipeline on **Confluent Cloud** with minimal setup.  
- Create a **MongoDB Atlas** vector index and query it via Flink.  
- Use **VoyageAI** to embed text into vectors for similarity search.  
- Run **`VECTOR_SEARCH`** in Flink SQL to power agentic/RAG‚Äëstyle lookups.  
- Watch data flow end‚Äëto‚Äëend‚Äî**no Docker, no complex services**.

<br>

## üìö What You Will Learn

- Quick setup of **Confluent Cloud**, **MongoDB Atlas**, and **VoyageAI** (UI + CLI).  
- How to create and use tables in **Flink SQL**.  
- How embeddings and vector indexes work together (VoyageAI ‚Üî MongoDB Atlas).  
- Patterns that are **simple to learn** and **easy to extend**.

<br>

## üèóÔ∏è Architecture Diagram

```
        +---------------------+
        |   Confluent Cloud   |   (UI + CLI for mgmt)
        +----------+----------+
                   |
                   v
             +-----+-----+
             |   Flink   |   (SQL, VECTOR_SEARCH)
             +-----+-----+
                   |
   +---------------+---------------+
   |                               |
   v                               v
+----------+                 +-------------+
| VoyageAI |  (Embeddings)   | MongoDB     |
|   API    +----------------> | Atlas       |
+----------+      vectors     | (Vector IX) |
                              +-------------+
```

<br>

## üîÑ Data Flow

**Flink SQL ‚Üí VoyageAI**  
Mock equipment maintenance text (IoT‚Äëish) is embedded via the VoyageAI API. The Python script writes messages (with vectors) into Kafka ‚Üí available to Flink SQL.

**Flink SQL ‚Üí MongoDB Atlas**  
Flink queries a MongoDB **external table** that points at a **vector index** and uses `VECTOR_SEARCH` to retrieve similar documents.

**Confluent Cloud UI/CLI ‚Üî Flink SQL**  
Use the Confluent UI or CLI to set up environments, topics, and a Flink compute pool, then launch the Flink SQL shell.

<br>

## üß™ Setup

### ‚úÖ Confluent Cloud (UI)

1) Sign up / sign in: https://confluent.cloud/  
2) In the UI, create:  
   - A **free environment**  
   - A **free Kafka cluster**  
   - A **Flink compute pool**

**üí¨ Pro tip ‚Äî Confluent AI Assistant:** In the UI, use the built‚Äëin assistant to fetch **Environment ID** / **Cluster ID** and generate CLI commands.

<table>
  <tr>
    <td>
      <a href="./images/create_cfltenvironment.jpg" target="_blank">
        <img src="./images/create_cfltenvironment.jpg" width="600"/>
      </a>
    </td>
  </tr>
  <tr>
    <td>
      <a href="./images/create_cfltenvironment2.jpg" target="_blank">
        <img src="./images/create_cfltenvironment2.jpg" width="300"/>
      </a>
    </td>
  </tr>
  <tr>
    <td>
      <a href="./images/create_cfltenvironment3.jpg" target="_blank">
        <img src="./images/create_cfltenvironment3.jpg" width="600"/>
      </a>
    </td>
  </tr>
  <tr>
    <td>
      <a href="./images/create_cfltenvironment4.jpg" target="_blank">
        <img src="./images/create_cfltenvironment4.jpg" width="600"/>
      </a>
    </td>
  </tr>
  <tr>
    <td>
      <a href="./images/create_cfltenvironment5.jpg" target="_blank">
        <img src="./images/create_cfltenvironment5.jpg" width="600"/>
      </a>
    </td>
  </tr>
  <tr>
    <td>
      <a href="./images/create_cfltenvironment_flink.jpg" target="_blank">
        <img src="./images/create_cfltenvironment_flink.jpg" width="800"/>
      </a>
    </td>
  </tr>
  <tr>
    <td>
      <a href="./images/create_cfltenvironment_flink1.jpg" target="_blank">
        <img src="./images/create_cfltenvironment_flink1.jpg" width="800"/>
      </a>
    </td>
  </tr>
  <tr>
    <td>
      <a href="./images/create_cfltenvironment_flink2.jpg" target="_blank">
        <img src="./images/create_cfltenvironment_flink2.jpg" width="600"/>
      </a>
    </td>
  </tr>
</table> 

### ‚úÖ MongoDB Atlas

1) Sign up: https://cloud.mongodb.com/  
2) Create a free cluster, then create:  
   ```text
   database: VoyageDemo
   collection: equipment_maintenance_externaldb_embeddings
   ```
3) Create a **vector index** (JSON editor), name: `equip_maint_vector_index`  
   ```json
   {
     "fields": [
       {
         "type": "vector",
         "path": "embeddings",
         "numDimensions": 1024,
         "similarity": "cosine"
       }
     ]
   }
   ```
   > Make sure `numDimensions` matches your VoyageAI model‚Äôs embedding size.

<table>
  <tr>
    <td>
      <a href="./images/mdb_createindex2.jpg" target="_blank">
        <img src="./images/mdb_createindex2.jpg" width="800"/>
      </a>
    </td>
  </tr>
  <tr>
    <td>
      <a href="./images/mdb_createindex.jpg" target="_blank">
        <img src="./images/mdb_createindex.jpg" width="800"/>
      </a>
    </td>
  </tr>
  <tr>
    <td>
      <a href="./images/mdb_viewindex_success.jpg" target="_blank">
        <img src="./images/mdb_viewindex_success.jpg" width="800"/>
      </a>
    </td>
  </tr>
</table> 

üí° Tip: double-check cloud provider/region alignment and network access/IP allowlist in Atlas.
For now you may want to test with an open Internet not just your IP address.

<table>
  <tr>
    <td>
      <a href="./images/create_mdbenvironment4.jpg" target="_blank">
        <img src="./images/create_mdbenvironment4.jpg" width="800"/>
      </a>
    </td>
  </tr>
</table> 

### ‚úÖ VoyageAI

- Create an API key from your VoyageAI dashboard (API Keys page).
- https://dashboard.voyageai.com/members
- Note your **model name** and **key** for the `.env` file.

<br>

### üîß CLI Tools

- Install **Confluent CLI**: https://docs.confluent.io/confluent-cli/current/install.html

<br>

## ‚öôÔ∏è Run It

### 1) Configure `.env`
Set your values (API keys, cluster/pool IDs, etc.).

### 2) Generate mock data
This creates a mock dataset in your MongoDB Atlas database

```bash
python mockdata_equipment_maintenance_data_generator.py
```
üìÑ Script: [`mockdata_equipment_maintenance_data_generator.py`](./mockdata_equipment_maintenance_data_generator.py)

### 3) Create the input table for user queries (using Flink SQL workspace)
This creates a kafka topic that will store your command line query/input and VoyageAI embedding of the same

In the Flink SQL workspace execute the following SQL:
```sql
CREATE TABLE user_query_embeddings (
  msg STRING,
  vector ARRAY<FLOAT>
);
```

<table>
  <tr>
    <td>
      <a href="./images/flink_sql_createtable_embeddings.jpg" target="_blank">
        <img src="./images/flink_sql_createtable_embeddings.jpg" width="1000"/>
      </a>
    </td>
  </tr>
</table> 

### 4) Run the user‚Äëquery input program (Python)
Produces `{msg, vector}` Avro messages to your topic for Flink to query using the new kafka topic created above.

```bash
python run_equipment_maintenance_query.py
# Example input when prompted:
# alarm system malfunction
# then type 'exit' to quit
```
üìÑ Script: [`run_equipment_maintenance_query.py`](./run_equipment_maintenance_query.py)

<table>
  <tr>
    <td>
      <a href="./images/shell_running_query_program.jpg" target="_blank">
        <img src="./images/shell_running_query_program.jpg"/>
      </a>
    </td>
  </tr>
</table> 

### 5) Create a MongoDB connection (Confluent UI)
Now that you have data in MongoDB Atlas and Confluent Cloud (via a kafka topic), you can query both using Flink SQL.

Let's add a MongoDB Integration that Flink SQL can use to connect to MongoDB Atlas (connection name example): `mongodb-equip-maintenance`  

Example endpoint form:
```
mongodb+srv://<your-cluster>.mongodb.net/?retryWrites=true&w=majority&appName=<YourAppName>
```

<table>
  <tr>
    <td>
      <a href="./images/cc_createmdb_integration4.jpg" target="_blank">
        <img src="./images/cc_createmdb_integration4.jpg" width="800"/>
      </a>
    </td>
  </tr>
  <tr>
    <td>
      <a href="./images/cc_createmdb_integration3.jpg" target="_blank">
        <img src="./images/cc_createmdb_integration3.jpg" width="800"/>
      </a>
    </td>
  </tr>
  <tr>
    <td>
      <a href="./images/cc_createmdb_integration2.jpg" target="_blank">
        <img src="./images/cc_createmdb_integration2.jpg" width="800"/>
      </a>
    </td>
  </tr> 
  <tr>
    <td>
      <a href="./images/cc_createmdb_integration.jpg" target="_blank">
        <img src="./images/cc_createmdb_integration.jpg" width="800"/>
      </a>
    </td>
  </tr>   
</table> 

### 6) Create an external table for MongoDB (Flink SQL)
Flink SQL will reference an external table that uses that new connection you just created.

```sql
CREATE TABLE mongodb (
  actions_taken   STRING,
  equipment_id    STRING,
  equipment_type  STRING,
  reported_issue  STRING,
  severity_level  STRING,
  embeddings      ARRAY<FLOAT>
) WITH (
  'connector'             = 'mongodb',
  'mongodb.connection'    = 'mongodb-equip-maintenance',
  'mongodb.database'      = 'VoyageDemo',
  'mongodb.collection'    = 'equipment_maintenance_externaldb_embeddings',
  'mongodb.index'         = 'equip_maint_vector_index',
  'mongodb.embedding_column' = 'embeddings',
  'mongodb.numCandidates' = '3'
);
```

<table>
  <tr>
    <td>
      <a href="./images/flink_sql_createtable_externaltablemongodb.jpg" target="_blank">
        <img src="./images/flink_sql_createtable_externaltablemongodb.jpg" width="1000"/>
      </a>
    </td>
  </tr>
</table> 

### 7) Run VECTOR_SEARCH (Flink SQL)
Finally, run a Flink SQL statement that joins the Confluent Cloud kafka topic that has your input query (and embedding) against the external MongoDB Atlas database that has your mock dataset to do a proper vector search.

```sql
SELECT msg, search_results
FROM user_query_embeddings,
  LATERAL TABLE(VECTOR_SEARCH(mongodb, 3, DESCRIPTOR(embeddings), user_query_embeddings.vector)) AS search_results;
```

Expected output (shape):
```
msg: alarm system malfunction
search_results: Replaced main pressure sensor, recalibrated alarm sensitivity, performed comprehensive system diagnostic,VT-01,Ventilator Pro,Intermittent alarm system malfunction during patient ventilation,High, ...
```

<table>
  <tr>
    <td>
      <a href="./images/flink_sql_running_vectorsearch_lateraltable.jpg" target="_blank">
        <img src="./images/flink_sql_running_vectorsearch_lateraltable.jpg" width="1000"/>
      </a>
    </td>
  </tr>
</table> 

<br>

## üß© Why VoyageAI + Flink `VECTOR_SEARCH`?

- **SQL‚Äënative** similarity search over external (MongoDB) vectors‚Äîsimple to operate.  
- **Fast, high‚Äëquality embeddings** from VoyageAI, easy to call from Python.  
- **Composable**: swap in your own topics, collections, indexes, and models later.  
- **Agent‚Äëready**: this is the retrieval backbone for agentic/RAG systems.

<br>

## üßØ Troubleshooting

- **Dimension mismatch**: Ensure `numDimensions` in the Mongo index equals your VoyageAI model‚Äôs vector size.  
- **Index not ready**: Wait for Atlas index build to finish before querying.  
- **Auth/connection errors**: Verify your MongoDB SRV string, IP allowlist, and Confluent connection name.  
- **Large messages**: If vectors get big, you may need to adjust Kafka/producer `max.request.size`.

<br>

## üßπ Teardown

Manually delete your MongoDB Atlas cluster and any remaining Confluent Cloud resources (clusters, topics, compute pools) to avoid charges.

<br>

## üìå Final Thoughts

This repo is intentionally lightweight and learning‚Äëfirst:
- Real‚Äëtime streaming + vector search + embeddings in one flow  
- Clear building blocks you can **reuse** and **automate** (Terraform, SDKs, CI) later  
- A great starting point before you dive into bigger systems

Happy hacking! üöÄ